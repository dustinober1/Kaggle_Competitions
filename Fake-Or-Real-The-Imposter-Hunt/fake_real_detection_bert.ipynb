{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f68a0b9",
   "metadata": {},
   "source": [
    "# Fake-or-Real Detection â€” Rebuilt Notebook\n",
    "\n",
    "This notebook reconstructs the original `fake_real_detection_bert.ipynb` using the project `process.md` as the source of truth. It contains environment checks, safe imports, a lightweight fallback predictor (TF-IDF + LogisticRegression) and hooks to run a Hugging Face transformer model if available. The goal is to provide a runnable, robust notebook for both quick smoke tests and full training/prediction when the environment has the required ML libraries.\n",
    "\n",
    "Sections:\n",
    "1. Environment and dependency checks\n",
    "2. Safe imports (installs if missing)\n",
    "3. Data loading helpers (reads `data/train` and `data/test` pairs)\n",
    "4. Simple baseline predictor (TF-IDF + LogisticRegression) for smoke tests\n",
    "5. Optional Hugging Face transformer prediction utilities (if `transformers` installed)\n",
    "6. Submission helper\n",
    "\n",
    "Note: This reconstructed notebook intentionally avoids long training runs. It includes code paths that gracefully fall back to lightweight methods if heavy packages or GPUs are unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796cff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section: Environment check\n",
    "import sys, os, platform\n",
    "print('Python:', sys.version.replace('\n",
    "',' '))\n",
    "print('Platform:', platform.platform())\n",
    "print('CWD:', os.getcwd())\n",
    "print('Notebook file:', __file__ if '__file__' in globals() else 'n/a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b195139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section: Safe imports with optional pip install fallback\n",
    "import importlib, subprocess, sys\n",
    "def safe_import(pkg, import_name=None, install_name=None):\n",
    "    import_name = import_name or pkg\n",
    "    install_name = install_name or pkg\n",
    "    try:\n",
    "        module = importlib.import_module(import_name)\n",
    "        print(f'Imported {import_name} (from {pkg})')\n",
    "        return module\n",
    "    except Exception as e:\n",
    "        print(f'Module {import_name} not found: {e}. Attempting to pip install {install_name}...')\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', install_name])\n",
    "        module = importlib.import_module(import_name)\n",
    "        print(f'Installed and imported {import_name}')\n",
    "        return module\n",
    "\n",
    "# Try lightweight packages first\n",
    "np = safe_import('numpy')\n",
    "pd = safe_import('pandas')\n",
    "sklearn = safe_import('sklearn', 'sklearn', 'scikit-learn')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Transformers and torch are optional; only installed if user allows heavy installs\n",
    "HAS_TRANSFORMERS = True\n",
    "try:\n",
    "    transformers = importlib.import_module('transformers')\n",
    "    torch = importlib.import_module('torch')\n",
    "    print('Transformers and torch available')\n",
    "except Exception as e:\n",
    "    print('Transformers/torch not available locally.')\n",
    "    HAS_TRANSFORMERS = False\n",
    "\n",
    "# Print versions\n",
    "import numpy as _np\n",
    "import pandas as _pd\n",
    "print('numpy', _np.__version__)\n",
    "print('pandas', _pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844d3fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section: Data helpers - locate data folder and read text pairs\n",
    "from pathlib import Path\n",
    "DATA_ROOT = Path('data')\n",
    "TRAIN_CSV = DATA_ROOT / 'train.csv'\n",
    "TRAIN_DIR = DATA_ROOT / 'train'\n",
    "TEST_DIR = DATA_ROOT / 'test'\n",
    "\n",
    "def read_pair_from_dir(article_dir):\n",
    "    # article_dir is a Path to a folder containing file_1.txt and file_2.txt\n",
    "    f1 = article_dir / 'file_1.txt'\n",
    "    f2 = article_dir / 'file_2.txt'\n",
    "    t1 = f1.read_text(encoding='utf-8') if f1.exists() else ''\n",
    "    t2 = f2.read_text(encoding='utf-8') if f2.exists() else ''\n",
    "    return t1, t2\n",
    "\n",
    "def preview_data(n=5):\n",
    "    if not TRAIN_DIR.exists():\n",
    "        print('No train directory found at', TRAIN_DIR)\n",
    "        return []\n",
    "    items = list(sorted(TRAIN_DIR.iterdir()))[:n]\n",
    "    rows = []\n",
    "    for p in items:\n",
    "        t1, t2 = read_pair_from_dir(p)\n",
    "        rows.append({'id': p.name, 'text1': t1[:400], 'text2': t2[:400]})\n",
    "    import pandas as pd\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Show preview (if data exists)\n",
    "preview_data(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03363330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section: Baseline predictor (TF-IDF + LogisticRegression) - suitable for smoke tests\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def build_baseline_model():\n",
    "    pipe = make_pipeline(TfidfVectorizer(max_features=20000, ngram_range=(1,2)),\n",
    "                         LogisticRegression(max_iter=1000))\n",
    "    return pipe\n",
    "\n",
    "def prepare_baseline_dataset(max_samples=2000):\n",
    "    # Read train.csv to obtain labels; fallback to building synthetic small dataset if missing\n",
    "    if TRAIN_CSV.exists():\n",
    "        df = pd.read_csv(TRAIN_CSV)\n",
    "        rows = []\n",
    "        for _, r in df.iterrows():\n",
    "            aid = r['id']\n",
    "            label = int(r['real_text_id'])\n",
    "            p = TRAIN_DIR / str(aid)\n",
    "            t1, t2 = read_pair_from_dir(p)\n",
    "            # Create two examples per article where each sample is a text labeled real/not\n",
    "            rows.append({'text': t1, 'label': 1 if label==1 else 0})\n",
    "            rows.append({'text': t2, 'label': 1 if label==2 else 0})\n",
    "        dataset = pd.DataFrame(rows)\n",
    "        if len(dataset) > max_samples:\n",
    "            dataset = dataset.sample(max_samples, random_state=0)\n",
    "        X = dataset['text'].fillna('')\n",
    "        y = dataset['label']\n",
    "        return X, y\n",
    "    else:\n",
    "        # Synthetic tiny dataset for smoke testing\n",
    "        texts = ['This is real news about finance and the economy.',\n",
    "                 'Clickbait fake news with false claims and sensational language.',\n",
    "                 'Official press release with verified quotes and data.',\n",
    "                 'Unverified rumors and fabricated stories.']\n",
    "        labels = [1,0,1,0]\n",
    "        return pd.Series(texts), pd.Series(labels)\n",
    "\n",
    "# Train baseline on small data and return the pipeline\n",
    "X, y = prepare_baseline_dataset(500)\n",
    "model = build_baseline_model()\n",
    "model.fit(X, y)\n",
    "print('Baseline model trained on', len(X), 'samples')\n",
    "\n",
    "# Example: predict for a pair\n",
    "def baseline_predict_pair(pipe, text1, text2):\n",
    "    probs = pipe.predict_proba([text1, text2])\n",
    "    # probs[:,1] is probability of being real\n",
    "    p1, p2 = probs[0,1], probs[1,1]\n",
    "    return 1 if p1>p2 else 2, max(p1,p2)\n",
    "\n",
    "print(baseline_predict_pair(model, 'Official report with numbers and quotes.', 'Sensational rumor with no source.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257819c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section: Optional Transformers-based utilities (only if available)\n",
    "HAS_HF = False\n",
    "try:\n",
    "    import transformers\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "    import torch\n",
    "    HAS_HF = True\n",
    "    print('Hugging Face transformers available:', transformers.__version__)\n",
    "except Exception as e:\n",
    "    print('Transformers not available or failed to import:', e)\n",
    "\n",
    "def load_transformer_model(model_path_or_name='bert-base-uncased', device=None):\n",
    "    if not HAS_HF:\n",
    "        raise RuntimeError('Transformers not available in this environment')\n",
    "    device = device or ('cuda' if torch.cuda.is_available() else ('mps' if getattr(torch,'has_mps',False) else 'cpu'))\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path_or_name)\n",
    "    model.to(device)\n",
    "    return model, tokenizer, device\n",
    "\n",
    "def hf_predict_pair(model, tokenizer, text1, text2, device='cpu'):\n",
    "    texts = [text1, text2]\n",
    "    enc = tokenizer(texts, truncation=True, padding='max_length', max_length=256, return_tensors='pt')\n",
    "    enc = {k:v.to(device) for k,v in enc.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**enc)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        # probs shape: (2,2) -> [sample, class] where class 1 is 'real' per training convention\n",
    "        p1, p2 = probs[0,1].item(), probs[1,1].item()\n",
    "        return (1 if p1>p2 else 2), max(p1,p2)\n",
    "\n",
    "print('Transformer utilities ready (if available)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8e5efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section: Submission helper (writes a CSV with id, real_text_id)\n",
    "def create_submission_from_df(predictions, out_path='bert_submission.csv'):\n",
    "    # predictions: list of dicts with keys id, real_text_id, confidence(optional)\n",
    "    out = pd.DataFrame(predictions)\n",
    "    out = out[['id','real_text_id']].copy()\n",
    "    out.to_csv(out_path, index=False)\n",
    "    print('Wrote submission to', out_path)\n",
    "\n",
    "# Example building predictions using baseline model on test dir (if exists)\n",
    "def predict_test_with_baseline(pipe, test_dir=TEST_DIR, limit=None):\n",
    "    preds = []\n",
    "    if not test_dir.exists():\n",
    "        print('No test dir at', test_dir); return preds\n",
    "    items = list(sorted(test_dir.iterdir()))\n",
    "    if limit: items = items[:limit]\n",
    "    for p in items:\n",
    "        t1, t2 = read_pair_from_dir(p)\n",
    "        best, conf = baseline_predict_pair(pipe, t1, t2)\n",
    "        preds.append({'id': p.name, 'real_text_id': best, 'confidence': float(conf)})\n",
    "    return preds\n",
    "\n",
    "print('Submission helper ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befad671",
   "metadata": {},
   "source": [
    "## How to run full training or transformer predictions\n",
    "\n",
    "- To run full Hugging Face training or prediction: ensure `transformers` and `torch` are installed, place model checkpoints under `results/BERT-Base-Uncased-Best` (or pass a model name like `google/electra-base-discriminator`) and use `load_transformer_model()`.\n",
    "- For quick smoke tests, use the baseline TF-IDF + LogisticRegression pipeline included above.\n",
    "\n",
    "If you'd like, I can now run the notebook's key cells (env check, imports, baseline training & a smoke prediction) to validate this rebuilt notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
